# Copy to docker.model.env and adjust as needed (docker.model.env is git-ignored).
SERVED_MODEL_NAME=autoglm-phone-9b
MODEL_REPO=zai-org/AutoGLM-Phone-9B
PORT=8000

# Optional: store HuggingFace cache on host disk (recommended to avoid filling Docker Desktop disk).
# Windows example:
# HF_CACHE_DIR=D:\huggingface_cache
#
# Optional: if you hit HF rate limits:
# HF_TOKEN=hf_xxx

# Optional tuning (24GB GPU friendly):
# GPU_MEMORY_UTILIZATION=0.85
# MAX_MODEL_LEN=16384
