services:
  vllm:
    image: vllm/vllm-openai:v0.12.0
    gpus: all
    shm_size: "8g"
    env_file:
      - ../docker.model.env
    ports:
      - "${PORT:-8000}:8000"
    volumes:
      - "${HF_CACHE_DIR:-../.cache/huggingface}:/root/.cache/huggingface"
    entrypoint: ["bash", "-lc"]
    command:
      - >-
        python3 -m vllm.entrypoints.openai.api_server
        --served-model-name ${SERVED_MODEL_NAME:-autoglm-phone-9b}
        --allowed-local-media-path /
        --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION:-0.9}
        --mm-encoder-tp-mode data
        --mm_processor_cache_type shm
        --mm_processor_kwargs "{\"max_pixels\":5000000}"
        --enforce-eager
        --max-model-len ${MAX_MODEL_LEN:-25480}
        --chat-template-content-format string
        --limit-mm-per-prompt "{\"image\":1,\"video\":0}"
        --model ${MODEL_REPO:-zai-org/AutoGLM-Phone-9B}
        --swap-space 1
        --port 8000
    # GPU is required. Ensure your Docker supports NVIDIA GPU (Docker Desktop GPU).
